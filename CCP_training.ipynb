{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOx7Q8Xs6svr",
        "outputId": "ea1a8c62-dd34-48da-a855-f8deafea3617"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set the paths to training and validation folders on Google Drive\n",
        "train_wf_path = '/content/drive/My Drive/BioSR/CCP/training_wf'\n",
        "train_gt_path = '/content/drive/My Drive/BioSR/CCP/training_gt'\n",
        "validate_wf_path = '/content/drive/My Drive/BioSR/CCP/validate_wf'\n",
        "validate_gt_path = '/content/drive/My Drive/BioSR/CCP/validate_gt'\n",
        "output_dir = '/content/drive/My Drive/BioSR/CCP/outputs'\n",
        "\n",
        "# Create output directory if not exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define image loading function\n",
        "def load_images_from_folder(folder):\n",
        "    images = []\n",
        "    for filename in os.listdir(folder):\n",
        "        img = Image.open(os.path.join(folder, filename)).convert('L')  # Convert to grayscale\n",
        "        img = transforms.ToTensor()(img)  # Convert image to tensor\n",
        "        images.append(img)\n",
        "    return images\n",
        "\n",
        "# Load WF and GT training images\n",
        "train_wf_images = load_images_from_folder(train_wf_path)\n",
        "train_gt_images = load_images_from_folder(train_gt_path)\n",
        "\n",
        "# Load WF and GT validation images\n",
        "validate_wf_images = load_images_from_folder(validate_wf_path)\n",
        "validate_gt_images = load_images_from_folder(validate_gt_path)\n"
      ],
      "metadata": {
        "id": "_dbWfOVg_fWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/JingyunLiang/SwinIR.git\n",
        "!pip install timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcyaeso__v7-",
        "outputId": "e7aaeab4-fcb3-44be-a110-f5f201171cde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SwinIR'...\n",
            "remote: Enumerating objects: 333, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 333 (delta 6), reused 5 (delta 2), pack-reused 320 (from 1)\u001b[K\n",
            "Receiving objects: 100% (333/333), 29.84 MiB | 20.10 MiB/s, done.\n",
            "Resolving deltas: 100% (119/119), done.\n",
            "Collecting timm\n",
            "  Downloading timm-1.0.9-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.19.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.24.7)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n",
            "Downloading timm-1.0.9-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: timm\n",
            "Successfully installed timm-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from SwinIR.models.network_swinir import SwinIR\n",
        "\n",
        "# Instantiate the model with your required configuration\n",
        "model = SwinIR(\n",
        "    upscale=2,  # Upscaling factor for 32x32 -> 64x64\n",
        "    in_chans=1,  # Grayscale input\n",
        "    img_size=32,  # WF input size of 32x32\n",
        "    window_size=8,  # Default window size\n",
        "    depths=[4, 4, 4, 4],  # Depth of each layer\n",
        "    embed_dim=120,  # Embedding dimension\n",
        "    num_heads=[4, 4, 4, 4],  # Number of attention heads\n",
        "    mlp_ratio=2,  # MLP ratio\n",
        "    upsampler='pixelshuffledirect',  # Upsampler for super-resolution\n",
        "    resi_connection='1conv'  # Residual connection\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laJ1OUzL_z4v",
        "outputId": "b9944141-bf18-4e9b-f0c5-1746f461216c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Define normalization function (Normalize images to [0, 1])\n",
        "def normalize(img):\n",
        "    return (img - img.min()) / (img.max() - img.min())\n",
        "\n",
        "# Define optimizer and learning rate\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-6)  # Lower learning rate\n",
        "\n",
        "# Use L1 Loss\n",
        "criterion = torch.nn.L1Loss()  # Switch to L1 loss\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "id": "LqulBtd7_44r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define normalization function (Normalize images to [0, 1])\n",
        "def normalize(img):\n",
        "    if img.max() == img.min():  # Check if the image is constant\n",
        "        return img  # Return the image unchanged if all values are the same\n",
        "    return (img - img.min()) / (img.max() - img.min())\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 10  # How many epochs to wait after last validation improvement\n",
        "best_val_loss = float('inf')  # Initialize best validation loss as infinity\n",
        "patience_counter = 0  # To keep track of how long we've gone without improvement\n",
        "\n",
        "# Training loop with early stopping\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for wf_img, gt_img in zip(train_wf_images, train_gt_images):\n",
        "        wf_img, gt_img = wf_img.to(device), gt_img.to(device)\n",
        "\n",
        "        # Normalize both WF and GT images\n",
        "        wf_img = normalize(wf_img)\n",
        "        gt_img = normalize(gt_img)\n",
        "\n",
        "        optimizer.zero_grad()  # Clear gradients for next backward pass\n",
        "\n",
        "        # Model inference\n",
        "        output = model(wf_img.unsqueeze(0))  # Add batch dimension\n",
        "\n",
        "        # Compute loss and normalize by output pixel count (64x64)\n",
        "        num_pixels = 64 * 64  # Output image has 64x64 pixels\n",
        "        loss = criterion(output, gt_img.unsqueeze(0)) / num_pixels  # Normalize by the number of pixels\n",
        "        loss.backward()  # Backpropagate the error\n",
        "\n",
        "        # Gradient clipping to avoid exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)  # More aggressive clipping\n",
        "\n",
        "        optimizer.step()  # Update model weights\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Print training loss for this epoch\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss/len(train_wf_images)}')\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():  # Disable gradient calculation for validation\n",
        "        for wf_img, gt_img in zip(validate_wf_images, validate_gt_images):\n",
        "            wf_img, gt_img = wf_img.to(device), gt_img.to(device)\n",
        "\n",
        "            # Normalize validation images\n",
        "            wf_img = normalize(wf_img)\n",
        "            gt_img = normalize(gt_img)\n",
        "\n",
        "            # Model inference\n",
        "            output = model(wf_img.unsqueeze(0))  # Add batch dimension\n",
        "\n",
        "            # Compute validation loss\n",
        "            loss = criterion(output, gt_img.unsqueeze(0)) / num_pixels  # Normalize by the number of pixels\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    # Average validation loss\n",
        "    val_loss = val_loss / len(validate_wf_images)\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Validation Loss: {val_loss}')\n",
        "\n",
        "    # Early stopping check\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss  # Update best validation loss\n",
        "        patience_counter = 0  # Reset patience counter\n",
        "        # Save the best model\n",
        "        torch.save(model.state_dict(), '/content/drive/My Drive/BioSR/CCP/swinir_best_model.pth')\n",
        "        print(\"Validation loss improved, model saved!\")\n",
        "    else:\n",
        "        patience_counter += 1  # Increment patience counter\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}. No improvement in validation loss for {patience} epochs.\")\n",
        "            break  # Stop training\n",
        "\n",
        "# Save the final trained model\n",
        "torch.save(model.state_dict(), '/content/drive/My Drive/BioSR/CCP/swinir_final_model.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoYUyZRnABMs",
        "outputId": "ba52e2b8-60cf-4f2d-af35-f209c85e31c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1000], Training Loss: 1.603611149569133e-05\n",
            "Epoch [1/1000], Validation Loss: 1.3928509093198551e-05\n",
            "Validation loss improved, model saved!\n",
            "Epoch [2/1000], Training Loss: 1.4020608224450371e-05\n",
            "Epoch [2/1000], Validation Loss: 1.3577499179165089e-05\n",
            "Validation loss improved, model saved!\n",
            "Epoch [3/1000], Training Loss: 1.3777634656042892e-05\n",
            "Epoch [3/1000], Validation Loss: 1.3424711267412527e-05\n",
            "Validation loss improved, model saved!\n",
            "Epoch [4/1000], Training Loss: 1.3641464552290472e-05\n",
            "Epoch [4/1000], Validation Loss: 1.3270514330593466e-05\n",
            "Validation loss improved, model saved!\n",
            "Epoch [5/1000], Training Loss: 1.3543990832005192e-05\n",
            "Epoch [5/1000], Validation Loss: 1.3186985070711268e-05\n",
            "Validation loss improved, model saved!\n",
            "Epoch [6/1000], Training Loss: 1.3478532500005259e-05\n",
            "Epoch [6/1000], Validation Loss: 1.317756565564802e-05\n",
            "Validation loss improved, model saved!\n",
            "Epoch [7/1000], Training Loss: 1.3423196314334973e-05\n",
            "Epoch [7/1000], Validation Loss: 1.3113094035411956e-05\n",
            "Validation loss improved, model saved!\n",
            "Epoch [8/1000], Training Loss: 1.3379661670990117e-05\n",
            "Epoch [8/1000], Validation Loss: 1.309955496215783e-05\n",
            "Validation loss improved, model saved!\n",
            "Epoch [9/1000], Training Loss: 1.3344710028735562e-05\n",
            "Epoch [9/1000], Validation Loss: 1.3065101122088485e-05\n",
            "Validation loss improved, model saved!\n",
            "Epoch [10/1000], Training Loss: 1.3314203702640271e-05\n",
            "Epoch [10/1000], Validation Loss: 1.301294245864584e-05\n",
            "Validation loss improved, model saved!\n",
            "Epoch [11/1000], Training Loss: 1.328412554927354e-05\n",
            "Epoch [11/1000], Validation Loss: 1.2997400375019828e-05\n",
            "Validation loss improved, model saved!\n",
            "Epoch [12/1000], Training Loss: 1.3266256904262264e-05\n",
            "Epoch [12/1000], Validation Loss: 1.2944316826077638e-05\n",
            "Validation loss improved, model saved!\n",
            "Epoch [13/1000], Training Loss: 1.3243071948559759e-05\n",
            "Epoch [13/1000], Validation Loss: 1.2931501370935115e-05\n",
            "Validation loss improved, model saved!\n",
            "Epoch [14/1000], Training Loss: 1.3225960697113237e-05\n",
            "Epoch [14/1000], Validation Loss: 1.301836070309845e-05\n",
            "Epoch [15/1000], Training Loss: 1.320760926994069e-05\n",
            "Epoch [15/1000], Validation Loss: 1.2946017306932037e-05\n",
            "Epoch [16/1000], Training Loss: 1.3194879955028683e-05\n",
            "Epoch [16/1000], Validation Loss: 1.290047469094841e-05\n",
            "Validation loss improved, model saved!\n",
            "Epoch [17/1000], Training Loss: 1.3182737327389304e-05\n",
            "Epoch [17/1000], Validation Loss: 1.292622398460076e-05\n",
            "Epoch [18/1000], Training Loss: 1.3174183781667637e-05\n",
            "Epoch [18/1000], Validation Loss: 1.2944957711561579e-05\n",
            "Epoch [19/1000], Training Loss: 1.316468952940113e-05\n",
            "Epoch [19/1000], Validation Loss: 1.284587832477276e-05\n",
            "Validation loss improved, model saved!\n",
            "Epoch [20/1000], Training Loss: 1.315845731436127e-05\n",
            "Epoch [20/1000], Validation Loss: 1.285628013019075e-05\n",
            "Epoch [21/1000], Training Loss: 1.3149668310167329e-05\n",
            "Epoch [21/1000], Validation Loss: 1.2875398850711564e-05\n",
            "Epoch [22/1000], Training Loss: 1.3143067914582218e-05\n",
            "Epoch [22/1000], Validation Loss: 1.2917087536645037e-05\n",
            "Epoch [23/1000], Training Loss: 1.313904061324962e-05\n",
            "Epoch [23/1000], Validation Loss: 1.2848770531535239e-05\n",
            "Epoch [24/1000], Training Loss: 1.3133396157573428e-05\n",
            "Epoch [24/1000], Validation Loss: 1.2907664999172792e-05\n",
            "Epoch [25/1000], Training Loss: 1.3130430591759534e-05\n",
            "Epoch [25/1000], Validation Loss: 1.2854085262195744e-05\n",
            "Epoch [26/1000], Training Loss: 1.3123344521046931e-05\n",
            "Epoch [26/1000], Validation Loss: 1.2882937848025398e-05\n",
            "Epoch [27/1000], Training Loss: 1.3118584897927121e-05\n",
            "Epoch [27/1000], Validation Loss: 1.2889033366087055e-05\n",
            "Epoch [28/1000], Training Loss: 1.312048509985999e-05\n",
            "Epoch [28/1000], Validation Loss: 1.2855963480989076e-05\n",
            "Epoch [29/1000], Training Loss: 1.311395958542505e-05\n",
            "Epoch [29/1000], Validation Loss: 1.2869426871715303e-05\n",
            "Early stopping at epoch 29. No improvement in validation loss for 10 epochs.\n"
          ]
        }
      ]
    }
  ]
}